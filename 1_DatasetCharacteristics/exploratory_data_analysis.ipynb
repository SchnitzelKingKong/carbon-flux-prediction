{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "\n",
    "## Table of Contents\n",
    "1. [Dataset Overview](#dataset-overview)\n",
    "2. [Handling Missing Values](#handling-missing-values)\n",
    "3. [Feature Distributions](#feature-distributions)\n",
    "4. [Possible Biases](#possible-biases)\n",
    "5. [Correlations](#correlations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ". [Correlations](#correlations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview\n",
    "\n",
    "[Provide a high-level overview of the dataset. This should include the source of the dataset, the number of samples, the number of features, and example showing the structure of the dataset.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Biovolume_per_site_class_and_depth.pkl - Contains biovolume data for each Profile_id and depth, separated into log-spaced size classes\n",
    "#       - Column names indicate size classes from 0.2 to ~10 mm equivalent spherical diameter (ESD)\n",
    "#       - Biovolume in mmÂ³ per liter seawater\n",
    "#       - Profile_id is always a string\n",
    "#       - depth in dbar (approximately equal to depth in meters) starts at 12.5 dbar and increases in 25 dbar steps to 987.5 dbar (40 depth levels)\n",
    "#       - depth_bin = integer from 0 to 39, indicating the depth level (0 = 12.5 dbar, 1 = 37.5 dbar, ..., 39 = 987.5 dbar)\n",
    "\n",
    "BV_data = pd.read_pickle('../Data/Biovolume_per_size_class_and_depth.pkl')\n",
    "BV_data.info()\n",
    "BV_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Environmental_data_depth_resolved.pkl - Contains metadata for every Profile_id and depth: \n",
    "#       - Latitude, Longitude & datetime (identical for all depths of one Profile_id)\n",
    "#       - 10 environmental variables from CopernicusMarine Services, interpolated to the sampling locations, times and depths\n",
    "#       - distance_to_coast_km (calculated from Latitude & Longitude)\n",
    "#       - MarineRegion (categorical variable, from MarineRegions shapefile)\n",
    "\n",
    "env_data = pd.read_pickle('../Data/Environmental_data_depth_resolved.pkl')\n",
    "# subset env_data to only contain Profile_ids present in BV_data\n",
    "env_data_subset = env_data[env_data['Profile_id'].isin(BV_data['Profile_id'])]\n",
    "env_data_subset.info()\n",
    "env_data.info()\n",
    "# save subsetted env_data for future use\n",
    "env_data_subset.to_pickle('../Data/Environmental_data_depth_resolved_subset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile_id_to_clusters.pkl - Contains mapping of Profile_id to cluster labels (KMeans clustering based on vertical patterns of environmental variables)\n",
    "#        - total of 10 clusters with unequal number of Profile_ids in each cluster\n",
    "cluster_mapping = pd.read_pickle('../Data/Profile_id_to_clusters.pkl')\n",
    "cluster_mapping.info()\n",
    "cluster_mapping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Number of samples = number of unique Profile_ids\n",
    "num_samples = BV_data[\"Profile_id\"].nunique()\n",
    "\n",
    "# Number of features = 17 size classes x 40 depth levels per Profile_id\n",
    "num_features = (BV_data.shape[1] - 3) * 40  # subtracting 3 for Profile_id and depth columns\n",
    "\n",
    "# Display these dataset characteristics\n",
    "print(f\"Number of samples: {num_samples}\")\n",
    "print(f\"Number of features: {num_features}\")\n",
    "\n",
    "# Display the first few rows of the dataframe to show the structure\n",
    "print(\"Example data:\")\n",
    "BV_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values\n",
    "\n",
    "[Identify any missing values in the dataset, and describe your approach to handle them if there are any. If there are no missing values simply indicate that there are none.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = BV_data.isnull().sum()\n",
    "print(missing_values)\n",
    "## --> There are no missing data in our dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values\n",
    "# Example: Replacing NaN values with the mean value of the column\n",
    "# df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Your code for handling missing values goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Distributions\n",
    "\n",
    "[Plot the distribution of various features and target variables. Comment on the skewness, outliers, or any other observations.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Plotting histograms of all numerical features\n",
    "#df.hist(figsize=(12, 12))\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Distribution of Raw Data (Features)\n",
    "\n",
    "\n",
    "Result -->\n",
    "All classes are extremely skewed to the right (skewness 8â€“55!).\n",
    "Raw data is completely unsuitable for normal distribution.\n",
    "Log transformation clearly improves the distribution.\n",
    "However, log normality is not perfect (due to huge n).\n",
    "The following applies to ALL classes: â€œmixed/unclear (log helps).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy import stats\n",
    "\n",
    "# 0) Explicitly use BV_data as the source DataFrame\n",
    "df = BV_data.copy()\n",
    "if df is None or df.empty:\n",
    "    raise ValueError(\"BV_data is not loaded or is empty. Load BV_data first.\")\n",
    "\n",
    "# 1) Select biovolume columns\n",
    "biovolume_cols = [col for col in df.columns if \"Biovolume [ppm]\" in col]\n",
    "\n",
    "print(f\"Found {len(biovolume_cols)} biovolume columns:\")\n",
    "for c in biovolume_cols:\n",
    "    print(\"  -\", c)\n",
    "\n",
    "\n",
    "def analyze_lognormality_with_report(df, cols, sample_size=5000):\n",
    "    results = []\n",
    "\n",
    "    for col in cols:\n",
    "        x = df[col].values\n",
    "        x_pos = x[x > 0]\n",
    "\n",
    "        if len(x_pos) < 50:\n",
    "            print(f\"\\n[INFO] Skipping column '{col}': too few positive values ({len(x_pos)}).\")\n",
    "            continue\n",
    "\n",
    "        # Subsample for Shapiro-Wilk\n",
    "        sample_raw = np.random.choice(x[~np.isnan(x)],\n",
    "                                      size=min(sample_size, len(x)),\n",
    "                                      replace=False)\n",
    "        sample_log = np.random.choice(x_pos,\n",
    "                                      size=min(sample_size, len(x_pos)),\n",
    "                                      replace=False)\n",
    "\n",
    "        skew_raw = stats.skew(sample_raw)\n",
    "        mean_raw = np.mean(sample_raw)\n",
    "        median_raw = np.median(sample_raw)\n",
    "\n",
    "        p_raw = stats.shapiro(sample_raw).pvalue if 3 <= len(sample_raw) <= 5000 else np.nan\n",
    "        p_log = stats.shapiro(np.log(sample_log)).pvalue if 3 <= len(sample_log) <= 5000 else np.nan\n",
    "\n",
    "        if np.isnan(p_raw) or np.isnan(p_log):\n",
    "            classification = \"undetermined\"\n",
    "            interp = \"The sample size is too large or too small for Shapiro-Wilk.\"\n",
    "        else:\n",
    "            if p_raw < 0.05 and p_log > 0.05:\n",
    "                classification = \"lognormal-like\"\n",
    "                interp = (\n",
    "                    \"Raw values strongly deviate from normality, but log-transformed values \"\n",
    "                    \"are compatible with a normal distribution. Geometric means and \"\n",
    "                    \"log-space CIs are appropriate.\"\n",
    "                )\n",
    "            elif p_raw < 0.05 and p_log < 0.05 and p_log > p_raw:\n",
    "                classification = \"mixed/unclear (log helps)\"\n",
    "                interp = (\n",
    "                    \"Both raw and log-transformed data deviate from perfect normality, \"\n",
    "                    \"but the log-transform clearly improves symmetry. Typical for heavily \"\n",
    "                    \"right-skewed ecological data.\"\n",
    "                )\n",
    "            elif p_raw > 0.05:\n",
    "                classification = \"normal-like (raw)\"\n",
    "                interp = (\n",
    "                    \"Raw data are roughly compatible with a normal distribution. \"\n",
    "                    \"Arithmetic means and symmetric CIs are acceptable.\"\n",
    "                )\n",
    "            else:\n",
    "                classification = \"mixed/unclear\"\n",
    "                interp = (\n",
    "                    \"Neither raw nor log-transformed values are close to normal. \"\n",
    "                    \"Consider median- or bootstrap-based statistics.\"\n",
    "                )\n",
    "\n",
    "        results.append({\n",
    "            \"column\": col,\n",
    "            \"n_total\": len(x),\n",
    "            \"n_positive\": len(x_pos),\n",
    "            \"mean_raw\": mean_raw,\n",
    "            \"median_raw\": median_raw,\n",
    "            \"skew_raw\": skew_raw,\n",
    "            \"p_raw_Shapiro\": p_raw,\n",
    "            \"p_log_Shapiro\": p_log,\n",
    "            \"classification\": classification\n",
    "        })\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 90)\n",
    "        print(f\"{col}\".upper())\n",
    "        print(\"=\" * 90)\n",
    "        print(f\"  Total observations       : {len(x)}\")\n",
    "        print(f\"  Positive values (>0)     : {len(x_pos)}\")\n",
    "        print(f\"  Mean (raw)               : {mean_raw:.4e}\")\n",
    "        print(f\"  Median (raw)             : {median_raw:.4e}\")\n",
    "        print(f\"  Skewness (raw)           : {skew_raw:.3f}\")\n",
    "        print(f\"  Shapiro p-value (raw)    : {p_raw:.3e}\")\n",
    "        print(f\"  Shapiro p-value (log)    : {p_log:.3e}\")\n",
    "        print(f\"  Classification           : {classification}\")\n",
    "        print(\"\\nInterpretation:\")\n",
    "        print(f\"  {interp}\")\n",
    "        print(\"-\" * 90)\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "summary_df = analyze_lognormality_with_report(df, biovolume_cols, sample_size=5000)\n",
    "\n",
    "print(\"\\n\\n=== SUMMARY TABLE (one row per size class) ===\")\n",
    "print(summary_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geometric mean biovolume per size class with 95% confidence intervals\n",
    "\n",
    "The biovolume data show strong right-skewness in all size classes (skewness 8â€“55) and clear deviations from normality (Shapiro p << 0.001). After log-transformation, the distributions become substantially more symmetric and closer to normal, making statistical analysis in log-space appropriate and robust.\n",
    "\n",
    "Therefore, geometric means are calculated for each size class (exp(mean(log(x)))), and 95% confidence intervals are computed in log-space and back-transformed to the original scale. The resulting confidence intervals are asymmetric, which correctly reflects the underlying data structure. A logarithmic y-axis is used because biovolume values span multiple orders of magnitude.\n",
    "\n",
    "\n",
    "Result -->\n",
    "The geometric mean biovolume increases systematically with particle size class, spanning more than one order of magnitude from the smallest to the largest ESD bins. This pattern reflects the expected scaling of particle or organismal biovolume with size. Confidence intervals are asymmetric and narrow for most classes, which is typical for log-normally distributed data with large sample sizes. Larger size classes show wider CIs due to fewer observations, increased variability, and a stronger influence of occasional large particles. Overall, the log-scale representation and geometric means reveal a consistent and well-structured sizeâ€“biovolume relationship across all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import re\n",
    "from matplotlib.ticker import LogFormatterSciNotation\n",
    "\n",
    "# Select biovolume columns\n",
    "biovolume_cols = [col for col in BV_data.columns if \"Biovolume [ppm]\" in col]\n",
    "\n",
    "# Positive values only\n",
    "bv = BV_data[biovolume_cols]\n",
    "bv_pos = bv.where(bv > 0)\n",
    "log_data = np.log(bv_pos)\n",
    "\n",
    "# Geometric mean = exp(mean(log(x)))\n",
    "mean_log = log_data.mean()\n",
    "std_log = log_data.std()\n",
    "n = log_data.count()\n",
    "\n",
    "# 95% CI in log-space\n",
    "t_crit = stats.t.ppf(0.975, df=n - 1)\n",
    "ci_half_width_log = t_crit * (std_log / np.sqrt(n))\n",
    "\n",
    "ci_low = np.exp(mean_log - ci_half_width_log)\n",
    "ci_high = np.exp(mean_log + ci_half_width_log)\n",
    "\n",
    "geo_mean = np.exp(mean_log)\n",
    "\n",
    "err_lower = geo_mean - ci_low\n",
    "err_upper = ci_high - geo_mean\n",
    "\n",
    "# Extract ESD labels\n",
    "size_classes = []\n",
    "for col in biovolume_cols:\n",
    "    m = re.search(r\"\\(ESD: *([^)]+)\\)\", col)\n",
    "    size_classes.append(m.group(1).strip() if m else col)\n",
    "\n",
    "# Build plot dataframe\n",
    "plot_df = pd.DataFrame({\n",
    "    \"Size class (ESD)\": size_classes,\n",
    "    \"Geo mean biovolume [ppm]\": geo_mean.values,\n",
    "    \"err_lower\": err_lower.values,\n",
    "    \"err_upper\": err_upper.values\n",
    "})\n",
    "\n",
    "# PLOT\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(18, 7))\n",
    "\n",
    "ax = sns.barplot(\n",
    "    data=plot_df,\n",
    "    x=\"Size class (ESD)\",\n",
    "    y=\"Geo mean biovolume [ppm]\",\n",
    "    color=\"steelblue\",\n",
    "    edgecolor=\"black\",\n",
    "    alpha=0.85,\n",
    "    errorbar=None\n",
    ")\n",
    "\n",
    "# Asymmetric CI\n",
    "yerr = np.vstack([plot_df.err_lower, plot_df.err_upper])\n",
    "plt.errorbar(\n",
    "    x=np.arange(len(plot_df)),\n",
    "    y=plot_df[\"Geo mean biovolume [ppm]\"],\n",
    "    yerr=yerr,\n",
    "    fmt=\"none\",\n",
    "    ecolor=\"black\",\n",
    "    capsize=4,\n",
    "    linewidth=1.2\n",
    ")\n",
    "\n",
    "ax.set_yscale(\"log\")\n",
    "ax.yaxis.set_major_formatter(LogFormatterSciNotation())\n",
    "\n",
    "ax.set_xlabel(\"Size class (ESD)\", fontsize=13)\n",
    "ax.set_ylabel(\"Geometric mean biovolume [ppm] (log scale)\", fontsize=13)\n",
    "ax.set_title(\"Geometric mean biovolume per size class (95% CI)\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geometric mean total biovolume per depth bin (95% CI)\n",
    "\n",
    "This analysis aggregates biovolume across all ESD size classes within each depth bin, producing a depth-resolved measure of total biovolume. Because the distribution is strongly right-skewed, geometric means and asymmetric 95% confidence intervals are computed in log-space, similar to the size-class analysis.\n",
    "\n",
    "\n",
    "Result -->\n",
    "The geometric mean of total biovolume is highest in the shallowest depth bins (0â€“2) and then declines steeply with depth, leveling off from roughly bin 8 onwards. This indicates a strong concentration of particulate/organismal biovolume in the upper water column, consistent with surfaceâ€driven production and decreasing biomass with depth. The 95% confidence intervals are relatively narrow across bins, suggesting that the vertical pattern is robust despite variability among profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Step 1: Select all biovolume columns ---\n",
    "biovolume_cols = [col for col in BV_data.columns if \"Biovolume [ppm]\" in col]\n",
    "\n",
    "# --- Step 2: Compute total biovolume per row (sum across ESD classes) ---\n",
    "BV_data[\"total_biovolume\"] = BV_data[biovolume_cols].clip(lower=0).sum(axis=1)\n",
    "\n",
    "# --- Step 3: Group by depth_bin ---\n",
    "grouped = BV_data.groupby(\"depth_bin\")[\"total_biovolume\"]\n",
    "\n",
    "# --- Step 4: Safely compute log-space stats per group ---\n",
    "# For each depth_bin, compute mean(log(x)), std(log(x)) and n = count(x>0).\n",
    "# Use these to build CI in log-space. Handle groups with n <= 1 to avoid len(float) errors.\n",
    "def group_log_stats(s):\n",
    "    pos = s[s > 0].astype(float)\n",
    "    npos = pos.size\n",
    "    if npos == 0:\n",
    "        return pd.Series({\"mean_log\": np.nan, \"std_log\": np.nan, \"n\": 0})\n",
    "    logv = np.log(pos)\n",
    "    # use sample std (ddof=1) if possible, otherwise 0.0\n",
    "    std = logv.std(ddof=1) if npos > 1 else 0.0\n",
    "    return pd.Series({\"mean_log\": logv.mean(), \"std_log\": std, \"n\": int(npos)})\n",
    "\n",
    "agg = grouped.apply(group_log_stats)\n",
    "\n",
    "# Ensure agg is a DataFrame with columns 'mean_log', 'std_log', 'n'.\n",
    "# Depending on pandas version/shape, apply() can return a Series with a multi-index;\n",
    "# normalize that into a clean DataFrame so accessing agg[\"n\"] won't raise KeyError.\n",
    "if isinstance(agg, pd.Series):\n",
    "    agg = agg.unstack()\n",
    "if isinstance(agg, pd.DataFrame) and agg.columns.nlevels > 1:\n",
    "    # collapse multi-level column names if present\n",
    "    agg.columns = [c[-1] if isinstance(c, tuple) else c for c in agg.columns]\n",
    "# ensure 'n' exists and is integer-like\n",
    "if \"n\" in agg.columns:\n",
    "    try:\n",
    "        agg[\"n\"] = agg[\"n\"].astype(int)\n",
    "    except Exception:\n",
    "        # fallback: coerce to numeric then to int, filling NaN with 0\n",
    "        agg[\"n\"] = pd.to_numeric(agg[\"n\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "# If no positive values in a group, leave NaNs and zeros; avoid invalid df for t.ppf\n",
    "df_n = agg[\"n\"]\n",
    "df_df = df_n - 1  # degrees of freedom\n",
    "\n",
    "# t critical values (will produce nan where df <= 0)\n",
    "t_crit = stats.t.ppf(0.975, df=df_df)\n",
    "\n",
    "# compute CI half width in log-space, set to 0 where std or n are not valid\n",
    "with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n",
    "    ci_half_width_log = t_crit * (agg[\"std_log\"] / np.sqrt(agg[\"n\"]))\n",
    "ci_half_width_log = ci_half_width_log.fillna(0.0)\n",
    "\n",
    "# compute CI bounds and geometric mean (back-transform)\n",
    "mean_log = agg[\"mean_log\"]\n",
    "ci_low  = np.exp(mean_log - ci_half_width_log)\n",
    "ci_high = np.exp(mean_log + ci_half_width_log)\n",
    "geo_mean = np.exp(mean_log)\n",
    "\n",
    "err_lower = geo_mean - ci_low\n",
    "err_upper = ci_high - geo_mean\n",
    "\n",
    "# --- Step 5: Build plotting dataframe ---\n",
    "plot_df = pd.DataFrame({\n",
    "    \"depth_bin\": agg.index.astype(int),\n",
    "    \"geo_mean\": geo_mean.values,\n",
    "    \"err_lower\": err_lower.values,\n",
    "    \"err_upper\": err_upper.values\n",
    "}).reset_index(drop=True)\n",
    "\n",
    "# --- Step 6: Plot ---\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(18, 7))\n",
    "\n",
    "ax = sns.barplot(\n",
    "    data=plot_df,\n",
    "    x=\"depth_bin\",\n",
    "    y=\"geo_mean\",\n",
    "    color=\"steelblue\",\n",
    "    edgecolor=\"black\",\n",
    "    alpha=0.85,\n",
    "    errorbar=None\n",
    ")\n",
    "\n",
    "# Asymmetric CI\n",
    "yerr = np.vstack([plot_df[\"err_lower\"], plot_df[\"err_upper\"]])\n",
    "plt.errorbar(\n",
    "    x=np.arange(len(plot_df)),\n",
    "    y=plot_df[\"geo_mean\"],\n",
    "    yerr=yerr,\n",
    "    fmt=\"none\",\n",
    "    ecolor=\"black\",\n",
    "    capsize=4,\n",
    "    linewidth=1.2\n",
    ")\n",
    "\n",
    "# Log y-scale\n",
    "ax.set_yscale(\"log\")\n",
    "ax.yaxis.set_major_formatter(LogFormatterSciNotation())\n",
    "\n",
    "ax.set_xlabel(\"Depth bin\", fontsize=13)\n",
    "ax.set_ylabel(\"Geometric mean total biovolume [ppm] (log scale)\", fontsize=13)\n",
    "ax.set_title(\"Geometric mean total biovolume per depth bin (95% CI)\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heatmap: Total biovolume per depth bin and size class\n",
    "\n",
    "Result -->\n",
    "\n",
    "The heatmap shows that total biovolume is strongly concentrated in the upper depth bins, with maximum values occurring in the top 0â€“3 bins. Biovolume decreases rapidly with depth, stabilizing at much lower levels below roughly bin 10. Across depth, smaller size classes dominate the biovolume signal, while larger particles contribute increasingly at shallower depths but become scarce with depth. This pattern reflects typical vertical structuring in particle or plankton communities, where production and larger particle abundance are highest near the surface and decline with depth due to sinking, remineralization, and reduced biological activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 1. Extract biovolume columns ---\n",
    "biovolume_cols = [c for c in BV_data.columns if \"Biovolume [ppm]\" in c]\n",
    "\n",
    "# Extract clean ESD labels\n",
    "size_labels = []\n",
    "for col in biovolume_cols:\n",
    "    m = re.search(r\"\\(ESD:\\s*([^)]+)\\)\", col)\n",
    "    size_labels.append(m.group(1) if m else col)\n",
    "\n",
    "# --- 2. Build matrix: depth_bin Ã— size_class ---\n",
    "# We sum biovolume *within each depth bin* for each ESD class\n",
    "heatmap_df = (\n",
    "    BV_data\n",
    "    .groupby(\"depth_bin\")[biovolume_cols]\n",
    "    .sum()                # sum biovolume per bin & size class\n",
    ")\n",
    "\n",
    "heatmap_df.columns = size_labels  # apply clean labels\n",
    "\n",
    "# --- 3. Plot heatmap ---\n",
    "plt.figure(figsize=(16, 10))\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "ax = sns.heatmap(\n",
    "    heatmap_df,\n",
    "    cmap=\"viridis\",\n",
    "    norm=plt.matplotlib.colors.LogNorm(),   # log colorscale (important!)\n",
    "    cbar_kws={\"label\": \"Total biovolume [ppm] (log scale)\"},\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Size class (ESD)\", fontsize=14)\n",
    "ax.set_ylabel(\"Depth bin\", fontsize=14)\n",
    "ax.set_title(\"Total biovolume per depth bin and size class\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outlier detection: Violinplots of biovolume per size class and depth bin\n",
    "\n",
    "\n",
    "Result -->\n",
    "\n",
    "Across all size classes and depth bins, the distributions exhibit pronounced positive skewness with persistent high-end outliers extending several orders of magnitude above the modal values. These outliers occur systematicallyâ€”particularly in the upper depth binsâ€”and form continuous heavy tails rather than isolated points, indicating that they reflect genuine ecological variability rather than measurement noise. Such extreme values are characteristic of particle and plankton size spectra, where sporadic large aggregates or organismal concentrations generate disproportionately high biovolume signals. Because a small number of extreme observations strongly inflates arithmetic statistics, the use of log-transformed data and geometric means provides a more appropriate and robust representation of central tendencies and variability within these inherently lognormal-like distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "import math\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Select biovolume columns\n",
    "# -----------------------------\n",
    "biovolume_cols = [c for c in BV_data.columns if \"Biovolume [ppm]\" in c]\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Compute log-space outliers\n",
    "# -----------------------------\n",
    "bv = BV_data[biovolume_cols]\n",
    "log_bv = np.log(bv.where(bv > 0))  \n",
    "\n",
    "Q1 = log_bv.quantile(0.25)\n",
    "Q3 = log_bv.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outlier_mask = (log_bv.lt(lower_bound)) | (log_bv.gt(upper_bound))\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Multi-panel violin + outliers\n",
    "# -----------------------------\n",
    "n_cols = 4\n",
    "n = len(biovolume_cols)\n",
    "n_rows = math.ceil(n / n_cols)\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "fig = plt.figure(figsize=(22, 4 * n_rows))\n",
    "\n",
    "for i, col in enumerate(biovolume_cols, start=1):\n",
    "    ax = plt.subplot(n_rows, n_cols, i)\n",
    "\n",
    "    temp = BV_data[[ \"depth_bin\", col ]].copy()\n",
    "    temp = temp[temp[col] > 0]\n",
    "\n",
    "    if temp.empty:\n",
    "        ax.text(0.5, 0.5, \"No positive values\",\n",
    "                ha='center', va='center', fontsize=10)\n",
    "        ax.set_axis_off()\n",
    "        continue\n",
    "\n",
    "    temp[\"is_outlier\"] = outlier_mask.loc[temp.index, col].fillna(False)\n",
    "\n",
    "    # Clean grey violin\n",
    "    sns.violinplot(\n",
    "        data=temp,\n",
    "        x=\"depth_bin\",\n",
    "        y=col,\n",
    "        density_norm=\"width\",\n",
    "        inner=\"quartile\",\n",
    "        cut=0,\n",
    "        linewidth=0.7,\n",
    "        color=\"lightgrey\",\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    #  Outliers highlighted\n",
    "    out = temp[temp[\"is_outlier\"]]\n",
    "    if not out.empty:\n",
    "        ax.scatter(\n",
    "            out[\"depth_bin\"],\n",
    "            out[col],\n",
    "            s=14,\n",
    "            color=\"red\",\n",
    "            alpha=0.8,\n",
    "            zorder=3\n",
    "        )\n",
    "\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # ðŸ”  **Improved X-axis labels**\n",
    "    # ---------------------------\n",
    "\n",
    "    # Option A: shorten labels\n",
    "    bins = sorted(temp[\"depth_bin\"].unique())\n",
    "    short_labels = [f\"Bin {b}\" for b in bins]\n",
    "    ax.set_xticks(bins)\n",
    "    ax.set_xticklabels(short_labels, rotation=60, ha=\"right\", fontsize=9)\n",
    "\n",
    "    # Title & axes\n",
    "    ax.set_title(col, fontsize=10)\n",
    "\n",
    "    if (i - 1) % n_cols != 0:\n",
    "        ax.set_ylabel(\"\")\n",
    "    else:\n",
    "        ax.set_ylabel(\"Biovolume [ppm] (log scale)\", fontsize=11)\n",
    "\n",
    "    if i <= (n_rows - 1) * n_cols:\n",
    "        ax.set_xlabel(\"\")\n",
    "    else:\n",
    "        ax.set_xlabel(\"Depth bins\", fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of target variables\n",
    "\n",
    "Result --> \n",
    "\n",
    "Arithmetic mean appropriate for: ph, no3, po4, o2, fe, thetao_mean, so_mean, distance_to_coast_km, si\n",
    "â†’ Skewness is small or moderate (|skew| < ~1â€“2), so linear scale is reasonable.\n",
    "\n",
    "Geometric mean appropriate for: chl, phyc\n",
    "â†’ Extremely right-skewed (skew > 5), strongly non-normal, strictly positive â†’ log-scale fits much better.\n",
    "\n",
    "Median vs. mean:\n",
    "Some variables (e.g., si, distance_to_coast_km) show large gaps between mean and median â†’ strong heterogeneity or long tails.\n",
    "\n",
    "Overall:\n",
    "Most environmental variables behave roughly symmetric enough for arithmetic means; only typical plankton-related variables (chl, phyc) show classic heavy-tailed patterns requiring geometric/log treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import shapiro, skew\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# ================================================================\n",
    "# 1) Select environmental variables (exclude coords & depth)\n",
    "# ================================================================\n",
    "\n",
    "vars_env = [\n",
    "    \"ph\", \"chl\", \"no3\", \"po4\", \"si\", \"o2\", \"fe\",\n",
    "    \"phyc\", \"thetao_mean\", \"so_mean\", \"distance_to_coast_km\"\n",
    "]\n",
    "\n",
    "env = env_data_subset[vars_env].copy()\n",
    "\n",
    "# ================================================================\n",
    "# 2) Statistical summary and dynamic \"recommended mean\" (labels only)\n",
    "# ================================================================\n",
    "\n",
    "results = []\n",
    "for col in vars_env:\n",
    "    x = env[col].dropna().values\n",
    "\n",
    "    # Shapiro-Wilk (subsample to â‰¤ 5000)\n",
    "    if len(x) > 5000:\n",
    "        x_sample = np.random.choice(x, 5000, replace=False)\n",
    "    else:\n",
    "        x_sample = x\n",
    "\n",
    "    try:\n",
    "        shap_p = shapiro(x_sample).pvalue\n",
    "    except:\n",
    "        shap_p = np.nan\n",
    "\n",
    "    # Skewness\n",
    "    skewness_value = skew(x)\n",
    "\n",
    "    # Decide on recommended mean TYPE (no numeric value) (2 as criterion is often used for enviromental data!)\n",
    "    if abs(skewness_value) < 2:\n",
    "        recommended_mean = \"arithmetic\"\n",
    "    else:\n",
    "        if np.all(x > 0):\n",
    "            recommended_mean = \"geometric\"\n",
    "        else:\n",
    "            recommended_mean = \"geometric (not applicable: non-positive values)\"\n",
    "\n",
    "    results.append({\n",
    "        \"variable\": col,\n",
    "        \"arith_mean\": np.mean(x),\n",
    "        \"median\": np.median(x),\n",
    "        \"std\": np.std(x, ddof=1),\n",
    "        \"skewness\": skewness_value,\n",
    "        \"Shapiro_p\": shap_p,\n",
    "        \"recommended_mean\": recommended_mean\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "stats_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n=== Statistical Summary of Environmental Variables ===\")\n",
    "display(stats_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean and CI of Target Variables\n",
    "\n",
    "Most variables show extremely tight 95% confidence intervals, meaning their mean estimates are very stable due to the large sample size (~224k). \n",
    "\n",
    "All arithmetic-mean variables display narrow, symmetric CIs in linear space, reflecting low relative uncertainty.\n",
    "\n",
    "For chl and phyc, the CIs were computed in log-space, producing asymmetric intervals after back-transformation â€” appropriate for their strongly right-skewed distributions. These two variables show the largest relative CI width, indicating higher variability compared to the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "from scipy.stats import t\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# =========================================================\n",
    "# 1) Define variables and split into arithmetic vs geometric\n",
    "# =========================================================\n",
    "\n",
    "# Variables for which an arithmetic mean is appropriate\n",
    "arith_vars = [\"ph\", \"no3\", \"po4\", \"o2\", \"thetao_mean\", \"fe\", \"si\", \"so_mean\", \"distance_to_coast_km\"]\n",
    "\n",
    "# Variables for which a geometric mean is more appropriate\n",
    "geo_vars = [\"chl\", \"phyc\"]\n",
    "\n",
    "all_vars = arith_vars + geo_vars\n",
    "env = env_data_subset[all_vars].copy()\n",
    "\n",
    "# =========================================================\n",
    "# 2) Helper functions to compute means and 95% CIs\n",
    "# =========================================================\n",
    "\n",
    "def arithmetic_mean_ci(x):\n",
    "    \"\"\"Arithmetic mean and symmetric 95% CI in original space.\"\"\"\n",
    "    x = x[~np.isnan(x)]\n",
    "    n = len(x)\n",
    "    mean = x.mean()\n",
    "    sd = x.std(ddof=1)\n",
    "    t_crit = t.ppf(0.975, df=n-1)\n",
    "    half_width = t_crit * sd / np.sqrt(n)\n",
    "    return mean, mean - half_width, mean + half_width, n\n",
    "\n",
    "def geometric_mean_ci(x):\n",
    "    \"\"\"Geometric mean and 95% CI computed in log-space and back-transformed.\"\"\"\n",
    "    x = x[x > 0]            # geometric mean requires strictly positive values\n",
    "    log_x = np.log(x)\n",
    "    n = len(log_x)\n",
    "    mean_log = log_x.mean()\n",
    "    sd_log = log_log = log_x.std(ddof=1)\n",
    "    t_crit = t.ppf(0.975, df=n-1)\n",
    "    half_width = t_crit * sd_log / np.sqrt(n)\n",
    "\n",
    "    mean = np.exp(mean_log)\n",
    "    low = np.exp(mean_log - half_width)\n",
    "    high = np.exp(mean_log + half_width)\n",
    "    return mean, low, high, n\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 4) Compute mean + CI for plotting (arith vs geom)\n",
    "# =========================================================\n",
    "\n",
    "plot_stats = {}\n",
    "for col in all_vars:\n",
    "    x = env[col].dropna().values\n",
    "\n",
    "    if col in geo_vars:\n",
    "        mean, low, high, n = geometric_mean_ci(x)\n",
    "        mean_type = \"geometric\"\n",
    "        log_scale = True\n",
    "        ci_note = \"95% CI in log-space\"\n",
    "    else:\n",
    "        mean, low, high, n = arithmetic_mean_ci(x)\n",
    "        mean_type = \"arithmetic\"\n",
    "        log_scale = False\n",
    "        ci_note = \"95% CI in linear space\"\n",
    "\n",
    "    plot_stats[col] = {\n",
    "        \"mean\": mean,\n",
    "        \"ci_low\": low,\n",
    "        \"ci_high\": high,\n",
    "        \"mean_type\": mean_type,\n",
    "        \"log_scale\": log_scale,\n",
    "        \"n\": n,\n",
    "        \"ci_note\": ci_note\n",
    "    }\n",
    "\n",
    "plot_stats_df = pd.DataFrame(plot_stats).T\n",
    "print(\"=== Plot statistics (mean + CI used in figure) ===\")\n",
    "display(plot_stats_df)\n",
    "\n",
    "# =========================================================\n",
    "# 5) Figure: bar charts with 95% CI, 4 subplots per row\n",
    "# =========================================================\n",
    "\n",
    "n_vars = len(all_vars)\n",
    "n_cols = 4\n",
    "n_rows = int(np.ceil(n_vars / n_cols))\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=n_rows,\n",
    "    ncols=n_cols,\n",
    "    figsize=(4.5 * n_cols, 3.5 * n_rows),\n",
    "    squeeze=False\n",
    ")\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, col in zip(axes, all_vars):\n",
    "    s = plot_stats[col]\n",
    "    mean = s[\"mean\"]\n",
    "    ci_low = s[\"ci_low\"]\n",
    "    ci_high = s[\"ci_high\"]\n",
    "    mean_type = s[\"mean_type\"]\n",
    "    log_scale = s[\"log_scale\"]\n",
    "    ci_note = s[\"ci_note\"]\n",
    "\n",
    "    # Narrow bar at x=0\n",
    "    x_pos = 0\n",
    "    width = 0.25  # narrower bar\n",
    "\n",
    "    ax.bar(\n",
    "        x_pos,\n",
    "        mean,\n",
    "        width=width,\n",
    "        color=\"steelblue\",\n",
    "        edgecolor=\"black\",\n",
    "        alpha=0.85\n",
    "    )\n",
    "\n",
    "    # Asymmetric error bar\n",
    "    yerr_lower = mean - ci_low\n",
    "    yerr_upper = ci_high - mean\n",
    "\n",
    "    ax.errorbar(\n",
    "        x_pos,\n",
    "        mean,\n",
    "        yerr=[[yerr_lower], [yerr_upper]],\n",
    "        fmt=\"none\",\n",
    "        ecolor=\"black\",\n",
    "        capsize=4,\n",
    "        linewidth=1.4\n",
    "    )\n",
    "\n",
    "    # Log scale for geometric means\n",
    "    if log_scale:\n",
    "        ax.set_yscale(\"log\")\n",
    "        ylabel = f\"{col} (log scale)\"\n",
    "    else:\n",
    "        ylabel = col\n",
    "\n",
    "    ax.set_ylabel(ylabel, fontsize=10)\n",
    "    ax.set_xticks([])\n",
    "\n",
    "    # Annotation box: mean type + CI info\n",
    "    ax.text(\n",
    "        0.03, 0.95,\n",
    "        f\"{mean_type.capitalize()} mean\\n{ci_note}\",\n",
    "        transform=ax.transAxes,\n",
    "        ha=\"left\",\n",
    "        va=\"top\",\n",
    "        fontsize=8.5,\n",
    "        color=\"dimgray\",\n",
    "        bbox=dict(boxstyle=\"round,pad=0.25\", fc=\"white\", ec=\"gray\", alpha=0.7)\n",
    "    )\n",
    "\n",
    "    # Sample size in lower-right corner\n",
    "    ax.text(\n",
    "        0.97, 0.05,\n",
    "        f\"n = {s['n']}\",\n",
    "        transform=ax.transAxes,\n",
    "        ha=\"right\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=8,\n",
    "        color=\"dimgray\"\n",
    "    )\n",
    "\n",
    "    ax.set_title(col, fontsize=11)\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Hide any unused axes\n",
    "for ax in axes[n_vars:]:\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Mean and 95% CI per environmental variable\\n\"\n",
    "    \"(arithmetic vs geometric mean indicated in each panel)\",\n",
    "    fontsize=14\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detection of Outlier in Target Variables\n",
    "\n",
    "Across most variables, the red points indicate a small number of extreme observations relative to the main data cloud. These outliers are most prominent in:\n",
    "\n",
    "chl and phyc â€“ extremely right-skewed, with many values close to zero and a long tail of high concentrations. Outliers here are expected and consistent with typical biological production spikes.\n",
    "\n",
    "si, so_mean, and distance_to_coast_km â€“ broader distributions with a few high-end extremes, likely representing specific environmental conditions (e.g., coastal vs. open ocean gradients).\n",
    "\n",
    "fe â€“ mostly tightly clustered but with a few isolated high values; common for trace metals with patchy inputs.\n",
    "\n",
    "Other variables (ph, no3, po4, o2, thetao_mean) show few outliers, and the red points lie close to the main density, suggesting stable distributions without extreme deviations.\n",
    "\n",
    "Overall, the violin plots confirm that only a few variables exhibit meaningful outlier-driven heterogeneity, mainly those with strong right skew (chl, phyc, si, distance_to_coast)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 1) Multi-subplot figure (2 rows, variables evenly distributed)\n",
    "# ================================================================\n",
    "\n",
    "n_cols = int(np.ceil(len(vars_env) / 2))\n",
    "n_rows = 2\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=n_rows,\n",
    "    ncols=n_cols,\n",
    "    figsize=(4 * n_cols, 10),\n",
    "    squeeze=False\n",
    ")\n",
    "axes = axes.flatten()\n",
    "\n",
    "# ================================================================\n",
    "# 2) Plot each variable\n",
    "# ================================================================\n",
    "\n",
    "for ax, col in zip(axes, vars_env):\n",
    "    x = env[col].dropna()\n",
    "\n",
    "    # Outlier detection (IQR)\n",
    "    Q1, Q3 = np.percentile(x, [25, 75])\n",
    "    IQR = Q3 - Q1\n",
    "    lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "    outliers = x[(x < lower) | (x > upper)]\n",
    "\n",
    "    # Log-scale decision\n",
    "    sk_val = skew(x)\n",
    "    use_log = (sk_val > 2) and (x > 0).all()\n",
    "\n",
    "    # Violin\n",
    "    sns.violinplot(\n",
    "        y=x,\n",
    "        orient=\"v\",\n",
    "        color=\"lightgray\",\n",
    "        inner=None,\n",
    "        linewidth=0,\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    # Outliers\n",
    "    ax.scatter(\n",
    "        np.zeros(len(outliers)),\n",
    "        outliers,\n",
    "        color=\"red\",\n",
    "        s=18,\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=0.3,\n",
    "        alpha=0.9\n",
    "    )\n",
    "\n",
    "    # Axis settings\n",
    "    if use_log:\n",
    "        ax.set_yscale(\"log\")\n",
    "        ax.set_ylabel(\"log scale\", fontsize=10)\n",
    "    else:\n",
    "        ax.set_ylabel(\"linear scale\", fontsize=10)\n",
    "\n",
    "    ax.set_xticks([])      # no x ticks\n",
    "    ax.set_xlabel(\"\")      # no x label\n",
    "\n",
    "    # Title: variable + skewness\n",
    "    ax.set_title(f\"{col} (skew={sk_val:.2f})\", fontsize=12)\n",
    "\n",
    "# Hide unused axes\n",
    "for ax in axes[len(vars_env):]:\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Biases\n",
    "\n",
    "[Investigate the dataset for any biases that could affect the modelâ€™s performance and fairness (e.g., class imbalance, historical biases).]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Checking for class imbalance in a classification problem\n",
    "# sns.countplot(x='target_variable', data=df)\n",
    "\n",
    "# Your code to investigate possible biases goes here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ###\n",
    "#       EXPLORE   :   Counts of cluster mappings across the subset of environment measurements.           #\n",
    "### ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ###\n",
    "\n",
    "cluster_merge_subset = env_data_subset.merge(cluster_mapping, on='Profile_id')\n",
    "cluster_merge_subset.head()\n",
    "sns.countplot(x='cluster', data=cluster_merge_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Class Biases\n",
    "\n",
    "There are **10 classes** in total, each cluster corresponds to one of these target classes.  \n",
    "We observe an imbalanced distribution of measurements across the respective classes.\n",
    "\n",
    "**The table below shows**;\n",
    "The class counts in this subset range from **10,360** to **42,880**, which appears rather substantial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ###\n",
    "#       EXPLORE   :   List of cluster counts across the subset of environment measurements.               #\n",
    "### ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ###\n",
    "class_occurances = cluster_merge_subset['cluster'].value_counts()\n",
    "class_df = class_occurances.to_frame(name='count').reset_index()\n",
    "class_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Variable Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ###\n",
    "#                   EXPLORE   :   Individual target variables and their distribution.                     #\n",
    "### ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ###\n",
    "\n",
    "cluster_merge_subset.hist(figsize=(24, 24))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-series evaluation of biases in sampling time and geographic dispersion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ###\n",
    "#       Explore :    Imbalances in measurement dates across study years.                                  #\n",
    "### ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ###\n",
    "cluster_merge_subset['year'] = cluster_merge_subset['datetime'].dt.year\n",
    "cluster_merge_subset['month'] = cluster_merge_subset['datetime'].dt.month\n",
    "\n",
    "heat = cluster_merge_subset.groupby(['year', 'month']).size().unstack(fill_value=0)\n",
    "sns.heatmap(heat, cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of samples per year and month\n",
    "\n",
    "This heatmap shows how sampling effort and model coverage vary across time. Darker cells represent months with a high number of available data points (either measured or modeled), while lighter areas indicate sparse or missing coverage. The plot highlights strong temporal heterogeneity: intensive coverage appears especially in 2021â€“2022, whereas earlier years and winter months contain far fewer data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ###\n",
    "#       EXPLORE :   Geographic distribution and site imbalance of sample records.                         #\n",
    "### ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ###\n",
    "\n",
    "df_merge = cluster_merge_subset.copy()\n",
    "df_merge['year'] = df_merge['datetime'].dt.year\n",
    "df_merge['month'] = df_merge['datetime'].dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latitudinal vs. longitudinal standard deviation\n",
    "\n",
    "**These two heatmaps illustrate how spatially dispersed the samples were in each month and year.**\n",
    "\n",
    "- The latitudinal standard deviation reflects how spread out the sampling was in the Northâ€“South direction. Higher values indicate that samples came from widely separated latitudes, suggesting sampling across multiple basins or depth gradients.\n",
    "\n",
    "- The longitudinal standard deviation measures spatial spread in the Eastâ€“West direction. High values here indicate sampling across geographically distant regions, often spanning different oceanic or coastal areas.\n",
    "\n",
    "**Comparing the two plots**:\n",
    "You can see that certain periods exhibit stronger dispersion in latitude (e.g., 2018, 2020â€“2023), while others show greater dispersion in longitude (e.g., 2014, 2021â€“2023). This difference suggests shifts in sampling strategy or geographic focus. For example, months with high latitudinal but low longitudinal variation likely represent northâ€“south transects, while the opposite pattern indicates eastâ€“west sampling campaigns. Together, the plots highlight how spatial sampling bias varies over time and direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_stats = (\n",
    "    df_merge\n",
    "    .groupby(['year', 'month'])['Latitude']\n",
    "    .agg(['min', 'max', 'std'])\n",
    ")\n",
    "lon_stats = (\n",
    "    df_merge\n",
    "    .groupby(['year', 'month'])['Longitude']\n",
    "    .agg(['min', 'max', 'std'])\n",
    ")\n",
    "\n",
    "lat_range = (lat_stats['max'] - lat_stats['min']).unstack(fill_value=0)\n",
    "lon_range = (lon_stats['max'] - lon_stats['min']).unstack(fill_value=0)\n",
    "\n",
    "lat_std = lat_stats['std'].unstack()\n",
    "lon_std = lon_stats['std'].unstack()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(lat_std, cmap='Reds')\n",
    "plt.title('Latitudinal standard deviation per year and month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Year')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(lon_std, cmap='Reds')\n",
    "plt.title('Longitudinal standard deviation per year and month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations\n",
    "\n",
    "[Explore correlations between features and the target variable, as well as among features themselves.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Plotting a heatmap to show feature correlations\n",
    "plt.figure(figsize=(12, 10))\n",
    "df = BV_data #Create a dataframe that only contains numerical values\n",
    "df = df.drop(columns=['Profile_id', \"depth\", \"depth_bin\"]) \n",
    "correlation_matrix = df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# --> The Biovolume of sizeclasses that are closely related have a high correlation, but the biovolume of very small and very large particle sizes is only poorly correlated\n",
    "# --> All values are positievly correlated. There is no negative correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Plotting a heatmap to show feature correlations\n",
    "plt.figure(figsize=(8, 6))\n",
    "df2 = env_data_subset #Create a dataframe that only contains numerical values\n",
    "df2 = df2.drop(columns=[\"Profile_id\", \"Latitude\", \"Longitude\", \"datetime\", \"depth\", \"MarineRegion\"]) \n",
    "correlation_matrix = df2.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# In the target values there is high correlation (>0.7) between:\n",
    "# ph & o2; chl & phyc; po4 & no3; si & no3; si & po4\n",
    "# There is high anticorrelation (<-0.7) between:\n",
    "# ph & no3; ph & po4; po4 & so_mean; \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge datasets due to their profile Id and their depth to see whether there are any correlations between biovolume (feature data) and environmental data (target data)\n",
    "merged_df = pd.merge(\n",
    "    BV_data, env_data_subset, \n",
    "    on=['Profile_id', 'depth'],  # SchlÃ¼sselspalten\n",
    "    how='left'                   # 'left', 'right', 'inner' oder 'outer'\n",
    ")\n",
    "#merged_df.head()\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Plotting a heatmap to show feature correlations\n",
    "plt.figure(figsize=(20, 15))\n",
    "merged_df = merged_df.drop(columns=['Profile_id', \"depth\", \"depth_bin\", \"Latitude\", \"Longitude\", \"datetime\", \"MarineRegion\"]) \n",
    "correlation_matrix = merged_df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# --> No real correlation between Biovolume (feature data) and environmental data (target data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine how biovolume values in each depth layer correlate with the biovolume data of the other depth layers\n",
    "biovolume_cols = [col for col in BV_data.columns if 'Biovolume' in col]\n",
    "\n",
    "plt.figure(figsize=(18, 14))\n",
    "# Pivotieren: Profile_id bleibt Index, depth wird zu Spalten\n",
    "df_pivot = BV_data.pivot(index='Profile_id', columns='depth', values=biovolume_cols[0])\n",
    "corr_depths = df_pivot.corr()  \n",
    "sns.heatmap(corr_depths, annot=True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# --> The results show that there is a very high correlation between depth. Has one biovolume class a high value in 12.5m it also has a high value in 100m, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
