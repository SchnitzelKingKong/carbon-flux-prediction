{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "\n",
    "## Table of Contents\n",
    "1. [Dataset Overview](#dataset-overview)\n",
    "2. [Handling Missing Values](#handling-missing-values)\n",
    "3. [Feature Distributions](#feature-distributions)\n",
    "4. [Possible Biases](#possible-biases)\n",
    "5. [Correlations](#correlations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ". [Correlations](#correlations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview\n",
    "\n",
    "[Provide a high-level overview of the dataset. This should include the source of the dataset, the number of samples, the number of features, and example showing the structure of the dataset.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Biovolume_per_site_class_and_depth.pkl - Contains biovolume data for each Profile_id and depth, separated into log-spaced size classes\n",
    "#       - Column names indicate size classes from 0.2 to ~10 mm equivalent spherical diameter (ESD)\n",
    "#       - Biovolume in mm³ per liter seawater\n",
    "#       - Profile_id is always a string\n",
    "#       - depth in dbar (approximately equal to depth in meters) starts at 12.5 dbar and increases in 25 dbar steps to 987.5 dbar (40 depth levels)\n",
    "#       - depth_bin = integer from 0 to 39, indicating the depth level (0 = 12.5 dbar, 1 = 37.5 dbar, ..., 39 = 987.5 dbar)\n",
    "\n",
    "BV_data = pd.read_pickle('../Data/Biovolume_per_size_class_and_depth.pkl')\n",
    "BV_data.info()\n",
    "BV_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Environmental_data_depth_resolved.pkl - Contains metadata for every Profile_id and depth: \n",
    "#       - Latitude, Longitude & datetime (identical for all depths of one Profile_id)\n",
    "#       - 10 environmental variables from CopernicusMarine Services, interpolated to the sampling locations, times and depths\n",
    "#       - distance_to_coast_km (calculated from Latitude & Longitude)\n",
    "#       - MarineRegion (categorical variable, from MarineRegions shapefile)\n",
    "\n",
    "env_data = pd.read_pickle('../Data/Environmental_data_depth_resolved.pkl')\n",
    "# subset env_data to only contain Profile_ids present in BV_data\n",
    "env_data_subset = env_data[env_data['Profile_id'].isin(BV_data['Profile_id'])]\n",
    "env_data_subset.info()\n",
    "env_data.info()\n",
    "# save subsetted env_data for future use\n",
    "env_data_subset.to_pickle('../Data/Environmental_data_depth_resolved_subset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile_id_to_clusters.pkl - Contains mapping of Profile_id to cluster labels (KMeans clustering based on vertical patterns of environmental variables)\n",
    "#        - total of 10 clusters with unequal number of Profile_ids in each cluster\n",
    "cluster_mapping = pd.read_pickle('../Data/Profile_id_to_clusters.pkl')\n",
    "cluster_mapping.info()\n",
    "cluster_mapping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Number of samples = number of unique Profile_ids\n",
    "num_samples = BV_data[\"Profile_id\"].nunique()\n",
    "\n",
    "# Number of features = 17 size classes x 40 depth levels per Profile_id\n",
    "num_features = (BV_data.shape[1] - 3) * 40  # subtracting 3 for Profile_id and depth columns\n",
    "\n",
    "# Display these dataset characteristics\n",
    "print(f\"Number of samples: {num_samples}\")\n",
    "print(f\"Number of features: {num_features}\")\n",
    "\n",
    "# Display the first few rows of the dataframe to show the structure\n",
    "print(\"Example data:\")\n",
    "BV_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values\n",
    "\n",
    "[Identify any missing values in the dataset, and describe your approach to handle them if there are any. If there are no missing values simply indicate that there are none.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = BV_data.isnull().sum()\n",
    "print(missing_values)\n",
    "## --> There are no missing data in our dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values\n",
    "# Example: Replacing NaN values with the mean value of the column\n",
    "# df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Your code for handling missing values goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Distributions\n",
    "\n",
    "[Plot the distribution of various features and target variables. Comment on the skewness, outliers, or any other observations.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Plotting histograms of all numerical features\n",
    "df.hist(figsize=(12, 12))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Biases\n",
    "\n",
    "[Investigate the dataset for any biases that could affect the model’s performance and fairness (e.g., class imbalance, historical biases).]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Checking for class imbalance in a classification problem\n",
    "# sns.countplot(x='target_variable', data=df)\n",
    "\n",
    "# Your code to investigate possible biases goes here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ###\n",
    "#       EXPLORE   :   Counts of cluster mappings across the subset of environment measurements.           #\n",
    "### ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ###\n",
    "\n",
    "cluster_merge_subset = env_data_subset.merge(cluster_mapping, on='Profile_id')\n",
    "cluster_merge_subset.head()\n",
    "sns.countplot(x='cluster', data=cluster_merge_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Class Biases\n",
    "\n",
    "There are **10 classes** in total, each cluster corresponds to one of these target classes.  \n",
    "We observe an imbalanced distribution of measurements across the respective classes.\n",
    "\n",
    "**The table below shows**;\n",
    "The class counts in this subset range from **10,360** to **42,880**, which appears rather substantial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ###\n",
    "#       EXPLORE   :   List of cluster counts across the subset of environment measurements.               #\n",
    "### ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ###\n",
    "class_occurances = cluster_merge_subset['cluster'].value_counts()\n",
    "class_df = class_occurances.to_frame(name='count').reset_index()\n",
    "class_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Variable Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ###\n",
    "#                   EXPLORE   :   Individual target variables and their distribution.                     #\n",
    "### ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ###\n",
    "\n",
    "cluster_merge_subset.hist(figsize=(24, 24))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-series evaluation of biases in sampling time and geographic dispersion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ###\n",
    "#       Explore :    Imbalances in measurement dates across study years.                                  #\n",
    "### ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ###\n",
    "cluster_merge_subset['year'] = cluster_merge_subset['datetime'].dt.year\n",
    "cluster_merge_subset['month'] = cluster_merge_subset['datetime'].dt.month\n",
    "\n",
    "heat = cluster_merge_subset.groupby(['year', 'month']).size().unstack(fill_value=0)\n",
    "sns.heatmap(heat, cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of samples per year and month\n",
    "\n",
    "This heatmap shows how sampling effort and model coverage vary across time. Darker cells represent months with a high number of available data points (either measured or modeled), while lighter areas indicate sparse or missing coverage. The plot highlights strong temporal heterogeneity: intensive coverage appears especially in 2021–2022, whereas earlier years and winter months contain far fewer data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ###\n",
    "#       EXPLORE :   Geographic distribution and site imbalance of sample records.                         #\n",
    "### ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ###\n",
    "\n",
    "df_merge = cluster_merge_subset.copy()\n",
    "df_merge['year'] = df_merge['datetime'].dt.year\n",
    "df_merge['month'] = df_merge['datetime'].dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latitudinal vs. longitudinal standard deviation\n",
    "\n",
    "**These two heatmaps illustrate how spatially dispersed the samples were in each month and year.**\n",
    "\n",
    "- The latitudinal standard deviation reflects how spread out the sampling was in the North–South direction. Higher values indicate that samples came from widely separated latitudes, suggesting sampling across multiple basins or depth gradients.\n",
    "\n",
    "- The longitudinal standard deviation measures spatial spread in the East–West direction. High values here indicate sampling across geographically distant regions, often spanning different oceanic or coastal areas.\n",
    "\n",
    "**Comparing the two plots**:\n",
    "You can see that certain periods exhibit stronger dispersion in latitude (e.g., 2018, 2020–2023), while others show greater dispersion in longitude (e.g., 2014, 2021–2023). This difference suggests shifts in sampling strategy or geographic focus. For example, months with high latitudinal but low longitudinal variation likely represent north–south transects, while the opposite pattern indicates east–west sampling campaigns. Together, the plots highlight how spatial sampling bias varies over time and direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_stats = (\n",
    "    df_merge\n",
    "    .groupby(['year', 'month'])['Latitude']\n",
    "    .agg(['min', 'max', 'std'])\n",
    ")\n",
    "lon_stats = (\n",
    "    df_merge\n",
    "    .groupby(['year', 'month'])['Longitude']\n",
    "    .agg(['min', 'max', 'std'])\n",
    ")\n",
    "\n",
    "lat_range = (lat_stats['max'] - lat_stats['min']).unstack(fill_value=0)\n",
    "lon_range = (lon_stats['max'] - lon_stats['min']).unstack(fill_value=0)\n",
    "\n",
    "lat_std = lat_stats['std'].unstack()\n",
    "lon_std = lon_stats['std'].unstack()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(lat_std, cmap='Reds')\n",
    "plt.title('Latitudinal standard deviation per year and month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Year')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(lon_std, cmap='Reds')\n",
    "plt.title('Longitudinal standard deviation per year and month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations\n",
    "\n",
    "[Explore correlations between features and the target variable, as well as among features themselves.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Plotting a heatmap to show feature correlations\n",
    "plt.figure(figsize=(12, 10))\n",
    "df = BV_data #Create a dataframe that only contains numerical values\n",
    "df = df.drop(columns=['Profile_id', \"depth\", \"depth_bin\"]) \n",
    "correlation_matrix = df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# --> The Biovolume of sizeclasses that are closely related have a high correlation, but the biovolume of very small and very large particle sizes is only poorly correlated\n",
    "# --> All values are positievly correlated. There is no negative correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Plotting a heatmap to show feature correlations\n",
    "plt.figure(figsize=(8, 6))\n",
    "df2 = env_data_subset #Create a dataframe that only contains numerical values\n",
    "df2 = df2.drop(columns=[\"Profile_id\", \"Latitude\", \"Longitude\", \"datetime\", \"depth\", \"MarineRegion\"]) \n",
    "correlation_matrix = df2.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# In the target values there is high correlation (>0.7) between:\n",
    "# ph & o2; chl & phyc; po4 & no3; si & no3; si & po4\n",
    "# There is high anticorrelation (<-0.7) between:\n",
    "# ph & no3; ph & po4; po4 & so_mean; \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge datasets due to their profile Id and their depth to see whether there are any correlations between biovolume (feature data) and environmental data (target data)\n",
    "merged_df = pd.merge(\n",
    "    BV_data, env_data_subset, \n",
    "    on=['Profile_id', 'depth'],  # Schlüsselspalten\n",
    "    how='left'                   # 'left', 'right', 'inner' oder 'outer'\n",
    ")\n",
    "#merged_df.head()\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Plotting a heatmap to show feature correlations\n",
    "plt.figure(figsize=(20, 15))\n",
    "merged_df = merged_df.drop(columns=['Profile_id', \"depth\", \"depth_bin\", \"Latitude\", \"Longitude\", \"datetime\", \"MarineRegion\"]) \n",
    "correlation_matrix = merged_df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# --> No real correlation between Biovolume (feature data) and environmental data (target data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine how biovolume values in each depth layer correlate with the biovolume data of the other depth layers\n",
    "biovolume_cols = [col for col in BV_data.columns if 'Biovolume' in col]\n",
    "\n",
    "plt.figure(figsize=(18, 14))\n",
    "# Pivotieren: Profile_id bleibt Index, depth wird zu Spalten\n",
    "df_pivot = BV_data.pivot(index='Profile_id', columns='depth', values=biovolume_cols[0])\n",
    "corr_depths = df_pivot.corr()  \n",
    "sns.heatmap(corr_depths, annot=True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# --> The results show that there is a very high correlation between depth. Has one biovolume class a high value in 12.5m it also has a high value in 100m, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
